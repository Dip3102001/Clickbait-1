{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7lO2gS7HlDHa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5233286f-7a34-41c2-f354-74e1a879bdea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import os;\n",
        "import os.path;\n",
        "\n",
        "import numpy as np;\n",
        "import pandas as pd;\n",
        "\n",
        "import nltk;\n",
        "import nltk.corpus;\n",
        "import nltk.tokenize;\n",
        "nltk.download('punkt');\n",
        "nltk.download('stopwords');"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def removePunctuation(text:list[str]):\n",
        "    buffer = [];\n",
        "    punc_list = [\"!\",\"#\",\"$\",\"%\",\"&\",\"(\",\")\",\"*\",\\\n",
        "                 \"+\",\"/\",\":\",\",\",\";\",\".\",\"<\",\"=\",\\\n",
        "                 \">\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"`\",\"{\",\\\n",
        "                 \"|\",\"}\",\"~\",\"\\t\",\"\\n\",\"-\",\"\\\"\",\"\\'\"];\n",
        "    for word in text:\n",
        "        if word not in punc_list:\n",
        "            buffer.append(word);\n",
        "\n",
        "    return buffer;"
      ],
      "metadata": {
        "id": "nR9NG_z3md0g"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def removeStopWords(text:list[str]):\n",
        "    buffer = [];\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'));\n",
        "    for word in stop_words:\n",
        "        if word not in text:\n",
        "            buffer.append(word);\n",
        "\n",
        "    return buffer;"
      ],
      "metadata": {
        "id": "Hgu17M67mfU-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepDataset(path_x=None,path_y=None):\n",
        "    if path_x is None or path_y is None:\n",
        "        raise Exception(\"Path_x or Path_y is not provided...\");\n",
        "    else:\n",
        "        X = [];\n",
        "        with open(path_x) as file:\n",
        "            lines = file.readlines();\n",
        "            for line in lines:\n",
        "                words = nltk.word_tokenize(line);\n",
        "                words = removePunctuation(words);\n",
        "                words = removeStopWords(words);\n",
        "                X.append(\" \".join(words).lower());\n",
        "\n",
        "        with open(path_y) as file:\n",
        "            y = file.readlines();\n",
        "\n",
        "        return X,[int(_) for _ in y];"
      ],
      "metadata": {
        "id": "9Q9VEBLCmd2t"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepTestset(path):\n",
        "    if path is None:\n",
        "        raise Exception(\"Path is expected..\");\n",
        "    else:\n",
        "        X = [];\n",
        "        with open(path) as file:\n",
        "            lines = file.readlines();\n",
        "            for line in lines:\n",
        "                words = nltk.word_tokenize(line);\n",
        "                words = removePunctuation(words);\n",
        "                words = removeStopWords(words);\n",
        "                X.append(\" \".join(words).lower());\n",
        "\n",
        "        return X;"
      ],
      "metadata": {
        "id": "S8wYil09md4-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x,train_y = prepDataset(\"/content/drive/MyDrive/Webis/DATA/trainset_cmb.txt\",\\\n",
        "                              \"/content/drive/MyDrive/Webis/y_train.txt\");\n",
        "val_x,val_y = prepDataset(\"/content/drive/MyDrive/Webis/DATA/valset_cmb.txt\",\\\n",
        "                            \"/content/drive/MyDrive/Webis/y_val.txt\");"
      ],
      "metadata": {
        "id": "BLZuHkmlmiEL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_x),len(train_y));\n",
        "print(len(val_x),len(val_y));"
      ],
      "metadata": {
        "id": "6ZTWtx_dmiGi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8c4af7a-ef55-43e6-8c56-9f7b3489f212"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3200 3200\n",
            "800 800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer;\n",
        "from sklearn.naive_bayes import MultinomialNB;\n",
        "from sklearn.metrics import accuracy_score;"
      ],
      "metadata": {
        "id": "gVqw1nQSmiJX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unigram\n",
        "vectorizer = CountVectorizer(ngram_range=(1,1));\n",
        "X_train = vectorizer.fit_transform(train_x);\n",
        "X_val = vectorizer.transform(val_x);\n",
        "\n",
        "classifier = MultinomialNB();\n",
        "classifier.fit(X_train,train_y);\n",
        "y_pred = classifier.predict(X_val);\n",
        "\n",
        "print(accuracy_score(val_y,y_pred));"
      ],
      "metadata": {
        "id": "t6O7qGmAmd7y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbaaf8c4-e104-4998-9b7e-4ece2dbd34a6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.42375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bigram\n",
        "vectorizer = CountVectorizer(ngram_range=(2,2));\n",
        "X_train = vectorizer.fit_transform(train_x);\n",
        "X_val = vectorizer.transform(val_x);\n",
        "\n",
        "classifier = MultinomialNB();\n",
        "classifier.fit(X_train,train_y);\n",
        "y_pred = classifier.predict(X_val);\n",
        "\n",
        "print(accuracy_score(val_y,y_pred));"
      ],
      "metadata": {
        "id": "0oQ-dyZomw2S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c5c4dad-33db-4c5c-defb-6e3319891cbc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.42625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unigram + bigram\n",
        "vectorizer = CountVectorizer(ngram_range=(1,2));\n",
        "X_train = vectorizer.fit_transform(train_x);\n",
        "X_val = vectorizer.transform(val_x);\n",
        "\n",
        "classifier = MultinomialNB();\n",
        "classifier.fit(X_train,train_y);\n",
        "y_pred = classifier.predict(X_val);\n",
        "\n",
        "print(accuracy_score(val_y,y_pred));"
      ],
      "metadata": {
        "id": "BZkJRSWYm_dw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d50773e-acab-4419-f62b-aa2af009e02a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tf-idf\n",
        "tf_vectorizer = TfidfVectorizer();\n",
        "X_train = tf_vectorizer.fit_transform(train_x);\n",
        "X_val = tf_vectorizer.transform(val_x);\n",
        "\n",
        "tf_classifier = MultinomialNB();\n",
        "tf_classifier.fit(X_train,train_y);\n",
        "y_pred = tf_classifier.predict(X_val);\n",
        "\n",
        "print(accuracy_score(val_y,y_pred));"
      ],
      "metadata": {
        "id": "g9JmFJvvnJF5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c848d408-62ee-4bcd-c766-4aa82750751b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.45125\n"
          ]
        }
      ]
    }
  ]
}