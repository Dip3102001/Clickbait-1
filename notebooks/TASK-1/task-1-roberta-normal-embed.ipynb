{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCgdgmDCiSmw"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, RobertaModel, AdamW;\n",
        "\n",
        "import torch;\n",
        "import torch.nn as nn;\n",
        "import torch.nn.functional as F;\n",
        "import torch.optim as optim;\n",
        "from torch.utils.data import Dataset, DataLoader;"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os;\n",
        "import os.path;"
      ],
      "metadata": {
        "id": "MTaOQDeQiezz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np;\n",
        "import matplotlib.pyplot as plt;\n",
        "\n",
        "import statistics;"
      ],
      "metadata": {
        "id": "wjmlA98mie2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu');\n",
        "print(device);"
      ],
      "metadata": {
        "id": "2NnEx8-9ie4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Using bert model for downstream classification task.\n",
        "\"\"\"\n",
        "\n",
        "class ROBERTaClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_classes):\n",
        "        super(ROBERTaClassifier, self).__init__();\n",
        "        self.bert = RobertaModel.from_pretrained(model_name);\n",
        "\n",
        "        # freezing model's parameters\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False;\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.2);\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes);\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask);\n",
        "        output = self.dropout(output.pooler_output);\n",
        "        output = self.fc(output);\n",
        "        return output;"
      ],
      "metadata": {
        "id": "l-c_WbeLie6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_model_name = 'FacebookAI/roberta-base';\n",
        "num_classes = 3;\n",
        "max_length = 512;\n",
        "batch_size = 16;\n",
        "epochs = 3;\n",
        "learning_rate = 1e-5;"
      ],
      "metadata": {
        "id": "HQ7k1gFzie82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(roberta_model_name);"
      ],
      "metadata": {
        "id": "NRO5YWI9ilZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(nn.Module):\n",
        "    def __init__(self,path_x,path_y,max_length):\n",
        "\n",
        "        self.max_length = max_length;\n",
        "\n",
        "        if not os.path.exists(path_x):\n",
        "            raise FileNotFoundError(path_x);\n",
        "        if not os.path.exists(path_y):\n",
        "            raise FileNotFoundError(path_y);\n",
        "\n",
        "        with open(path_x,'r') as f:\n",
        "            self.x = f.readlines();\n",
        "\n",
        "        with open(path_y,'r') as f:\n",
        "            self.y = f.readlines();\n",
        "\n",
        "        if len(self.x) != len(self.y):\n",
        "            raise ValueError(\"x and y must have same length\");\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x);\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        x = self.x[idx];\n",
        "        y = self.y[idx];\n",
        "        output = tokenizer(x, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt');\n",
        "        return {\n",
        "            'input_ids': output['input_ids'].flatten(),\n",
        "            'attention_mask': output['attention_mask'].flatten(),\n",
        "            'label' : torch.tensor(int(self.y[idx]))\n",
        "        };"
      ],
      "metadata": {
        "id": "a8RtomttimN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset('/content/drive/MyDrive/Webis/DATA/rev_trainset_cmb.txt',\n",
        "                        '/content/drive/MyDrive/Webis/y_train.txt',max_length);\n",
        "\n",
        "val_dataset = Dataset('/content/drive/MyDrive/Webis/DATA/rev_valset_cmb.txt',\n",
        "                      '/content/drive/MyDrive/Webis/y_val.txt',max_length);\n",
        "\n",
        "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True);\n",
        "val_loader = DataLoader(val_dataset,batch_size=batch_size,shuffle=True);"
      ],
      "metadata": {
        "id": "yfBUPuvkiobH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ROBERTaClassifier(roberta_model_name, num_classes);\n",
        "model = model.to(device);\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate);\n",
        "criterion = nn.CrossEntropyLoss();"
      ],
      "metadata": {
        "id": "1z1RZC4aiody"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_best_path = \"/content/drive/MyDrive/Webis/PARAM/\";"
      ],
      "metadata": {
        "id": "1wcmH6t1imRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5;\n",
        "\n",
        "train_loss = [];\n",
        "val_loss = [];\n",
        "train_acc = [];\n",
        "val_acc = [];\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    train_loss_epoch = [];\n",
        "    val_loss_epoch = [];\n",
        "    train_acc_epoch = [];\n",
        "    val_acc_epoch = [];\n",
        "\n",
        "    model.train();\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad();\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device);\n",
        "        attention_mask = batch['attention_mask'].to(device);\n",
        "        y = batch['label'].to(device);\n",
        "\n",
        "        output = model(input_ids,attention_mask);\n",
        "        loss = criterion(output,y);\n",
        "        loss.backward();\n",
        "        optimizer.step();\n",
        "\n",
        "        train_loss_epoch.append(loss.item());\n",
        "\n",
        "        accuracy = torch.argmax(output,dim=-1).view(-1) == y.view(-1);\n",
        "        train_acc_epoch.append((torch.sum(accuracy) / len(accuracy)).item());\n",
        "\n",
        "\n",
        "    model.eval();\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device);\n",
        "            attention_mask = batch['attention_mask'].to(device);\n",
        "            y = batch['label'].to(device);\n",
        "\n",
        "            output = model(input_ids,attention_mask);\n",
        "            loss = criterion(output,y);\n",
        "\n",
        "            val_loss_epoch.append(loss.item());\n",
        "\n",
        "            accuracy = torch.argmax(output,dim=-1).view(-1) == y.view(-1);\n",
        "            val_acc_epoch.append((torch.sum(accuracy) / len(accuracy)).item());\n",
        "\n",
        "\n",
        "    print(f\"Epoch[{epoch}][{epochs}] : Training Loss :{statistics.mean(train_loss_epoch)}, Validation Loss :{statistics.mean(val_loss_epoch)}, \\\n",
        "Training Accuracy :{statistics.mean(train_acc_epoch)}, Validation Accuracy :{statistics.mean(val_acc_epoch)}\");\n",
        "\n",
        "    if len(val_loss) == 0 or statistics.mean(val_loss_epoch) < min(val_loss):\n",
        "        torch.save(model.state_dict(),store_best_path + 'roberta_embed.pt');\n",
        "\n",
        "    train_loss.append(statistics.mean(train_loss_epoch));\n",
        "    val_loss.append(statistics.mean(val_loss_epoch));\n",
        "    train_acc.append(statistics.mean(train_acc_epoch));\n",
        "    val_acc.append(statistics.mean(val_acc_epoch));"
      ],
      "metadata": {
        "id": "_ckiJuqCimTx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}