{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M8sontoPXKW",
        "outputId": "6303135f-93da-4b2e-d3db-f2df77dac111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AscdcMcfNzep"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel, AdamW;\n",
        "\n",
        "import torch;\n",
        "import torch.nn as nn;\n",
        "import torch.nn.functional as F;\n",
        "import torch.optim as optim;\n",
        "from torch.utils.data import Dataset, DataLoader;"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os;\n",
        "import os.path;"
      ],
      "metadata": {
        "id": "-e_pP74qN6o2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np;\n",
        "import matplotlib.pyplot as plt;\n",
        "\n",
        "import statistics;"
      ],
      "metadata": {
        "id": "jfkpy4hFN6rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu');\n",
        "print(device);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN9b-ZozN6yv",
        "outputId": "a9779bad-d567-4728-da29-1c5c49862645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Using bert model for downstream classification task.\n",
        "\"\"\"\n",
        "\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_classes):\n",
        "        super(BERTClassifier, self).__init__();\n",
        "        self.bert = BertModel.from_pretrained(model_name);\n",
        "        self.dropout = nn.Dropout(p=0.2);\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes);\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask);\n",
        "        output = self.dropout(output.pooler_output);\n",
        "        output = self.fc(output);\n",
        "        return output;"
      ],
      "metadata": {
        "id": "CznWbpK7N60w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model_name = 'bert-base-uncased';\n",
        "num_classes = 3;\n",
        "max_length = 512;\n",
        "batch_size = 16;\n",
        "epochs = 3;\n",
        "learning_rate = 1e-5;"
      ],
      "metadata": {
        "id": "W2-YSdg1N63D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name);"
      ],
      "metadata": {
        "id": "Oc8OAAuXOB4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(nn.Module):\n",
        "    def __init__(self,path_x,path_y,max_length):\n",
        "\n",
        "        self.max_length = max_length;\n",
        "\n",
        "        if not os.path.exists(path_x):\n",
        "            raise FileNotFoundError(path_x);\n",
        "        if not os.path.exists(path_y):\n",
        "            raise FileNotFoundError(path_y);\n",
        "\n",
        "        with open(path_x,'r') as f:\n",
        "            self.x = f.readlines();\n",
        "\n",
        "        with open(path_y,'r') as f:\n",
        "            self.y = f.readlines();\n",
        "\n",
        "        if len(self.x) != len(self.y):\n",
        "            raise ValueError(\"x and y must have same length\");\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x);\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        x = self.x[idx];\n",
        "        y = self.y[idx];\n",
        "        output = tokenizer(x, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt');\n",
        "        return {\n",
        "            'input_ids': output['input_ids'].flatten(),\n",
        "            'attention_mask': output['attention_mask'].flatten(),\n",
        "            'label' : torch.tensor(int(self.y[idx]))\n",
        "        };"
      ],
      "metadata": {
        "id": "lFgQ6KhROL6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset('/content/drive/MyDrive/Data/DATA/rev_trainset_cmb.txt',\n",
        "                        '/content/drive/MyDrive/Data/DATA/y_train.txt',max_length);\n",
        "\n",
        "val_dataset = Dataset('/content/drive/MyDrive/Data/DATA/rev_valset_cmb.txt',\n",
        "                      '/content/drive/MyDrive/Data/DATA/y_val.txt',max_length);\n",
        "\n",
        "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True);\n",
        "val_loader = DataLoader(val_dataset,batch_size=batch_size,shuffle=True);"
      ],
      "metadata": {
        "id": "-LSjtx45ORE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERTClassifier(bert_model_name, num_classes);\n",
        "model = model.to(device);\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate);\n",
        "criterion = nn.CrossEntropyLoss();"
      ],
      "metadata": {
        "id": "VZ-BTOQoOMHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_best_path = \"/content/drive/MyDrive/PARAM/\";"
      ],
      "metadata": {
        "id": "5oW0LmtHPOJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2GuIvJ6LQs9",
        "outputId": "1f3d075e-fee1-4d26-b28d-3be27208d51d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[0][5] : Training Loss :1.009646703004837, Validation Loss :0.9755233526229858, Training Accuracy :0.454375, Validation Accuracy :0.4525\n",
            "Epoch[1][5] : Training Loss :0.9425327682495117, Validation Loss :0.9520699071884156, Training Accuracy :0.521875, Validation Accuracy :0.5225\n",
            "Epoch[2][5] : Training Loss :0.8549542713165283, Validation Loss :0.9754091727733613, Training Accuracy :0.6015625, Validation Accuracy :0.50875\n",
            "Epoch[3][5] : Training Loss :0.6947817783057689, Validation Loss :1.0235264921188354, Training Accuracy :0.7190625, Validation Accuracy :0.53125\n",
            "Epoch[4][5] : Training Loss :0.48276638604700567, Validation Loss :1.196674988269806, Training Accuracy :0.8278125, Validation Accuracy :0.51375\n"
          ]
        }
      ],
      "source": [
        "epochs = 5;\n",
        "\n",
        "train_loss = [];\n",
        "val_loss = [];\n",
        "train_acc = [];\n",
        "val_acc = [];\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    train_loss_epoch = [];\n",
        "    val_loss_epoch = [];\n",
        "    train_acc_epoch = [];\n",
        "    val_acc_epoch = [];\n",
        "\n",
        "    model.train();\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad();\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device);\n",
        "        attention_mask = batch['attention_mask'].to(device);\n",
        "        y = batch['label'].to(device);\n",
        "\n",
        "        output = model(input_ids,attention_mask);\n",
        "        loss = criterion(output,y);\n",
        "        loss.backward();\n",
        "        optimizer.step();\n",
        "\n",
        "        train_loss_epoch.append(loss.item());\n",
        "\n",
        "        accuracy = torch.argmax(output,dim=-1).view(-1) == y.view(-1);\n",
        "        train_acc_epoch.append((torch.sum(accuracy) / len(accuracy)).item());\n",
        "\n",
        "\n",
        "    model.eval();\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device);\n",
        "            attention_mask = batch['attention_mask'].to(device);\n",
        "            y = batch['label'].to(device);\n",
        "\n",
        "            output = model(input_ids,attention_mask);\n",
        "            loss = criterion(output,y);\n",
        "\n",
        "            val_loss_epoch.append(loss.item());\n",
        "\n",
        "            accuracy = torch.argmax(output,dim=-1).view(-1) == y.view(-1);\n",
        "            val_acc_epoch.append((torch.sum(accuracy) / len(accuracy)).item());\n",
        "\n",
        "\n",
        "    print(f\"Epoch[{epoch}][{epochs}] : Training Loss :{statistics.mean(train_loss_epoch)}, Validation Loss :{statistics.mean(val_loss_epoch)}, \\\n",
        "Training Accuracy :{statistics.mean(train_acc_epoch)}, Validation Accuracy :{statistics.mean(val_acc_epoch)}\");\n",
        "\n",
        "    if len(val_loss) == 0 or statistics.mean(val_loss_epoch) < min(val_loss):\n",
        "        torch.save(model.state_dict(),store_best_path + 'bert_rouge.pt');\n",
        "\n",
        "    train_loss.append(statistics.mean(train_loss_epoch));\n",
        "    val_loss.append(statistics.mean(val_loss_epoch));\n",
        "    train_acc.append(statistics.mean(train_acc_epoch));\n",
        "    val_acc.append(statistics.mean(val_acc_epoch));"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x1rWOXutUNj_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}