{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFPmguizeflw"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, RobertaModel, AdamW;\n",
        "\n",
        "import torch;\n",
        "import torch.nn as nn;\n",
        "import torch.nn.functional as F;\n",
        "import torch.optim as optim;\n",
        "from torch.utils.data import Dataset, DataLoader;"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os;\n",
        "import os.path;"
      ],
      "metadata": {
        "id": "5JcdaoM3erD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np;\n",
        "import matplotlib.pyplot as plt;\n",
        "\n",
        "import statistics;"
      ],
      "metadata": {
        "id": "AJZWXaj8es9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu');\n",
        "print(device);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hh4LTHoGeuRO",
        "outputId": "3e6bca2d-e17f-4735-b287-b95291d32e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Using bert model for downstream classification task.\n",
        "\"\"\"\n",
        "\n",
        "class ROBERTaClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_classes):\n",
        "        super(ROBERTaClassifier, self).__init__();\n",
        "        self.bert = RobertaModel.from_pretrained(model_name);\n",
        "\n",
        "        # freezing model's parameters\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False;\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.2);\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes);\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask);\n",
        "        output = self.dropout(output.pooler_output);\n",
        "        output = self.fc(output);\n",
        "        return output;"
      ],
      "metadata": {
        "id": "S9lvxsjUev6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_model_name = 'FacebookAI/roberta-base';\n",
        "num_classes = 3;\n",
        "max_length = 512;\n",
        "batch_size = 16;\n",
        "epochs = 3;\n",
        "learning_rate = 1e-3;"
      ],
      "metadata": {
        "id": "zOpy1KZSeyun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(roberta_model_name);"
      ],
      "metadata": {
        "id": "JVnjyriIez6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(nn.Module):\n",
        "    def __init__(self,path_x,path_y,max_length):\n",
        "\n",
        "        self.max_length = max_length;\n",
        "\n",
        "        if not os.path.exists(path_x):\n",
        "            raise FileNotFoundError(path_x);\n",
        "        if not os.path.exists(path_y):\n",
        "            raise FileNotFoundError(path_y);\n",
        "\n",
        "        with open(path_x,'r') as f:\n",
        "            self.x = f.readlines();\n",
        "\n",
        "        with open(path_y,'r') as f:\n",
        "            self.y = f.readlines();\n",
        "\n",
        "        if len(self.x) != len(self.y):\n",
        "            raise ValueError(\"x and y must have same length\");\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x);\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        x = self.x[idx];\n",
        "        y = self.y[idx];\n",
        "        output = tokenizer(x, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt');\n",
        "        return {\n",
        "            'input_ids': output['input_ids'].flatten(),\n",
        "            'attention_mask': output['attention_mask'].flatten(),\n",
        "            'label' : torch.tensor(int(self.y[idx]))\n",
        "        };"
      ],
      "metadata": {
        "id": "SZsqWwNJe2D3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset('/content/drive/MyDrive/Webis/DATA/rev_trainset_cmb.txt',\n",
        "                        '/content/drive/MyDrive/Webis/y_train.txt',max_length);\n",
        "\n",
        "val_dataset = Dataset('/content/drive/MyDrive/Webis/DATA/rev_valset_cmb.txt',\n",
        "                      '/content/drive/MyDrive/Webis/y_val.txt',max_length);\n",
        "\n",
        "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True);\n",
        "val_loader = DataLoader(val_dataset,batch_size=batch_size,shuffle=True);"
      ],
      "metadata": {
        "id": "7hQCoHcde4Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ROBERTaClassifier(roberta_model_name, num_classes);\n",
        "model = model.to(device);\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate);\n",
        "criterion = nn.CrossEntropyLoss();"
      ],
      "metadata": {
        "id": "Si0Gp9B3e5Ld",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f70ebf23-beb3-4aeb-91de-0965465f199c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "store_best_path = \"/content/drive/MyDrive/Webis/PARAM/\";"
      ],
      "metadata": {
        "id": "qUPM1_MMe5OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20;\n",
        "\n",
        "train_loss = [];\n",
        "val_loss = [];\n",
        "train_acc = [];\n",
        "val_acc = [];\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    train_loss_epoch = [];\n",
        "    val_loss_epoch = [];\n",
        "    train_acc_epoch = [];\n",
        "    val_acc_epoch = [];\n",
        "\n",
        "    model.train();\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad();\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device);\n",
        "        attention_mask = batch['attention_mask'].to(device);\n",
        "        y = batch['label'].to(device);\n",
        "\n",
        "        output = model(input_ids,attention_mask);\n",
        "        loss = criterion(output,y);\n",
        "        loss.backward();\n",
        "        optimizer.step();\n",
        "\n",
        "        train_loss_epoch.append(loss.item());\n",
        "\n",
        "        accuracy = torch.argmax(output,dim=-1).view(-1) == y.view(-1);\n",
        "        train_acc_epoch.append((torch.sum(accuracy) / len(accuracy)).item());\n",
        "\n",
        "\n",
        "    model.eval();\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device);\n",
        "            attention_mask = batch['attention_mask'].to(device);\n",
        "            y = batch['label'].to(device);\n",
        "\n",
        "            output = model(input_ids,attention_mask);\n",
        "            loss = criterion(output,y);\n",
        "\n",
        "            val_loss_epoch.append(loss.item());\n",
        "\n",
        "            accuracy = torch.argmax(output,dim=-1).view(-1) == y.view(-1);\n",
        "            val_acc_epoch.append((torch.sum(accuracy) / len(accuracy)).item());\n",
        "\n",
        "\n",
        "    print(f\"Epoch[{epoch}][{epochs}] : Training Loss :{statistics.mean(train_loss_epoch)}, Validation Loss :{statistics.mean(val_loss_epoch)}, \\\n",
        "Training Accuracy :{statistics.mean(train_acc_epoch)}, Validation Accuracy :{statistics.mean(val_acc_epoch)}\");\n",
        "\n",
        "    if len(val_loss) == 0 or statistics.mean(val_loss_epoch) < min(val_loss):\n",
        "        torch.save(model.state_dict(),store_best_path + 'roberta_rouge_embed.pt');\n",
        "\n",
        "    train_loss.append(statistics.mean(train_loss_epoch));\n",
        "    val_loss.append(statistics.mean(val_loss_epoch));\n",
        "    train_acc.append(statistics.mean(train_acc_epoch));\n",
        "    val_acc.append(statistics.mean(val_acc_epoch));"
      ],
      "metadata": {
        "id": "OlgcYsOwe5Q0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3a403b0-6c41-495c-bed0-2d6e5371bbd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[0][20] : Training Loss :1.0548809367418288, Validation Loss :1.0491487562656403, Training Accuracy :0.4090625, Validation Accuracy :0.41875\n",
            "Epoch[1][20] : Training Loss :1.0501217657327653, Validation Loss :1.0437435472011567, Training Accuracy :0.416875, Validation Accuracy :0.4025\n",
            "Epoch[2][20] : Training Loss :1.045251303911209, Validation Loss :1.0774966073036194, Training Accuracy :0.4184375, Validation Accuracy :0.4025\n",
            "Epoch[3][20] : Training Loss :1.0402333766222, Validation Loss :1.0358317053318025, Training Accuracy :0.441875, Validation Accuracy :0.41875\n",
            "Epoch[4][20] : Training Loss :1.0373432144522667, Validation Loss :1.0315304481983185, Training Accuracy :0.425625, Validation Accuracy :0.47375\n",
            "Epoch[5][20] : Training Loss :1.0367929968237877, Validation Loss :1.0301452350616456, Training Accuracy :0.43375, Validation Accuracy :0.455\n",
            "Epoch[6][20] : Training Loss :1.0373430383205413, Validation Loss :1.0274482774734497, Training Accuracy :0.4346875, Validation Accuracy :0.46\n",
            "Epoch[7][20] : Training Loss :1.0343837118148804, Validation Loss :1.0349592983722686, Training Accuracy :0.4409375, Validation Accuracy :0.41875\n",
            "Epoch[8][20] : Training Loss :1.034616942703724, Validation Loss :1.0252882206439973, Training Accuracy :0.434375, Validation Accuracy :0.47375\n",
            "Epoch[9][20] : Training Loss :1.0334067350625993, Validation Loss :1.0242086017131806, Training Accuracy :0.443125, Validation Accuracy :0.47375\n",
            "Epoch[10][20] : Training Loss :1.026443452835083, Validation Loss :1.0297248148918152, Training Accuracy :0.4515625, Validation Accuracy :0.40375\n",
            "Epoch[11][20] : Training Loss :1.030230032801628, Validation Loss :1.032693293094635, Training Accuracy :0.438125, Validation Accuracy :0.4025\n",
            "Epoch[12][20] : Training Loss :1.0284231734275817, Validation Loss :1.0233954095840454, Training Accuracy :0.450625, Validation Accuracy :0.4325\n",
            "Epoch[13][20] : Training Loss :1.0341523531079293, Validation Loss :1.0290607607364655, Training Accuracy :0.428125, Validation Accuracy :0.41875\n",
            "Epoch[14][20] : Training Loss :1.0277558967471123, Validation Loss :1.0211888813972474, Training Accuracy :0.4559375, Validation Accuracy :0.4625\n",
            "Epoch[15][20] : Training Loss :1.0205149728059768, Validation Loss :1.022076836824417, Training Accuracy :0.45375, Validation Accuracy :0.47125\n",
            "Epoch[16][20] : Training Loss :1.0267425933480263, Validation Loss :1.0279013347625732, Training Accuracy :0.4478125, Validation Accuracy :0.40375\n",
            "Epoch[17][20] : Training Loss :1.0226210752129554, Validation Loss :1.0169760608673095, Training Accuracy :0.439375, Validation Accuracy :0.46375\n",
            "Epoch[18][20] : Training Loss :1.0217606967687607, Validation Loss :1.0203565442562104, Training Accuracy :0.449375, Validation Accuracy :0.42875\n",
            "Epoch[19][20] : Training Loss :1.0170537075400352, Validation Loss :1.0353345012664794, Training Accuracy :0.4575, Validation Accuracy :0.41875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SKRNyc2LH9H7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}